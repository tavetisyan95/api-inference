{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc66533-bf07-4c8f-b617-0141ea8f8087",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5b7205-df80-44aa-a142-4e099fc2467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary tools\n",
    "from flask import Flask\n",
    "from flask_restful import Resource, Api, reqparse\n",
    "from flask_cors import CORS\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7e17186-3c39-4564-a90b-0d7f93c9efd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-tweet-emotion.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Instantiating our DistilBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-tweet-emotion\", num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82048ccb-cd2e-4d4b-ab36-2957a5783db2",
   "metadata": {},
   "source": [
    "## Building a processing function for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47cf784c-f500-4ac3-996d-8d88c46aeb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dictionary with class names for conversion\n",
    "class_names = {0: \"anger\", 1: \"joy\", 2: \"optimism\", 3: \"sadness\"}\n",
    "\n",
    "# A function containing the transformation steps from above\n",
    "def logits_to_class_names(predictions):\n",
    "    predictions = tf.nn.softmax(predictions.logits)\n",
    "    predictions = tf.argmax(predictions, axis=1).numpy()\n",
    "    predictions = [class_names[prediction] for prediction in predictions]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494ba9e5-4f04-479a-b15f-0662e29d0db0",
   "metadata": {},
   "source": [
    "## Building the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e2831-409d-47bf-bb03-ce0cb93d7300",
   "metadata": {},
   "source": [
    "### Creating a Flask application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7289155f-0454-4d53-827d-a9b70b94f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a Flask application\n",
    "app = Flask(import_name=__name__)\n",
    "CORS(app)\n",
    "api = Api(app=app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5188bbac-677c-48a8-b4c3-f02140cf200e",
   "metadata": {},
   "source": [
    "### Defining arguments for HTTP requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c06df7-5891-4b09-92df-8a85247646bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flask_restful.reqparse.RequestParser at 0x2428a324e20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = reqparse.RequestParser()\n",
    "parser.add_argument(name=\"Sequences\", type=str, action=\"append\",\n",
    "                    help=\"The sequence to be classified\", required=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c1371-49ae-497b-a48f-86a30c82187f",
   "metadata": {},
   "source": [
    "### Building an endpoint for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6cb6cc7-a392-4cc2-8d89-533eeaee24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class to represent our endpoint\n",
    "class Inference(Resource):\n",
    "    # A method corresponding to a GET request\n",
    "    def get(self):\n",
    "        # Parsing the arguments we defined earlier\n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        # Tokenizing the sequence\n",
    "        sequence = tokenizer(args[\"Sequences\"], return_tensors=\"tf\", padding=True)\n",
    "        \n",
    "        # Obtaining a prediction\n",
    "        prediction = logits_to_class_names(model(sequence))\n",
    "        \n",
    "        # Returning the prediction\n",
    "        return {\"Predictions\": prediction}, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27cf51c-8a00-4363-918d-b7f77b3a4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the endpoint to our app\n",
    "api.add_resource(Inference, \"/inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44aa12-6f19-4321-b94b-dd19ba7f4355",
   "metadata": {},
   "source": [
    "### Launching our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c481e1-1707-43fd-bfc8-9329be3f5607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [26/Oct/2021 12:47:21] \"GET /inference HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Oct/2021 12:47:21] \"GET /inference HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Oct/2021 12:47:52] \"GET /inference HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Oct/2021 12:47:53] \"GET /inference HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Oct/2021 12:48:12] \"GET /inference HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Oct/2021 12:48:13] \"GET /inference HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# launching our app\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
